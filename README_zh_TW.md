# CogVideoX å·¥å»  ğŸ§ª

[Read in English](./README.md) | [ä¸­æ–‡é˜…è¯»](./README_zh.md)

åœ¨ 24GB GPU è¨˜æ†¶é«”ä¸‹å° Cog ç³»åˆ—è¦–é »æ¨¡å‹é€²è¡Œå¾®èª¿ä»¥å¯¦ç¾å®¢è£½åŒ–è¦–é »ç”Ÿæˆï¼Œæ”¯æŒå¤šè§£æåº¦ âš¡ï¸ğŸ“¼

<table align="center">
<tr>
  <td align="center"><video src="https://github.com/user-attachments/assets/aad07161-87cb-4784-9e6b-16d06581e3e5">æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æŒè¦–é »æ¨™ç±¤ã€‚</video></td>
</tr>
</table>

## å¿«é€Ÿé–‹å§‹

å…‹éš†æ­¤å„²å­˜åº«ä¸¦ç¢ºä¿å®‰è£äº†ç›¸é—œä¾è³´ï¼š`pip install -r requirements.txt`ã€‚

æ¥è‘—ä¸‹è¼‰è³‡æ–™é›†ï¼š

```
# å®‰è£ `huggingface_hub`
huggingface-cli download   --repo-type dataset Wild-Heart/Disney-VideoGeneration-Dataset   --local-dir video-dataset-disney
```

ç„¶å¾Œå•Ÿå‹• LoRA å¾®èª¿é€²è¡Œæ–‡æœ¬åˆ°è¦–é »çš„ç”Ÿæˆï¼ˆæ ¹æ“šæ‚¨çš„é¸æ“‡ä¿®æ”¹ä¸åŒçš„è¶…åƒæ•¸ã€è³‡æ–™é›†æ ¹ç›®éŒ„ä»¥åŠå…¶ä»–é…ç½®é¸é …ï¼‰ï¼š

```
# å° CogVideoX æ¨¡å‹é€²è¡Œæ–‡æœ¬åˆ°è¦–é »çš„ LoRA å¾®èª¿
./train_text_to_video_lora.sh

# å° CogVideoX æ¨¡å‹é€²è¡Œæ–‡æœ¬åˆ°è¦–é »çš„å®Œæ•´å¾®èª¿
./train_text_to_video_sft.sh

# å° CogVideoX æ¨¡å‹é€²è¡Œåœ–åƒåˆ°è¦–é »çš„ LoRA å¾®èª¿
./train_image_to_video_lora.sh
```

å‡è¨­æ‚¨çš„ LoRA å·²ä¿å­˜ä¸¦æ¨é€åˆ° HF Hubï¼Œä¸¦å‘½åç‚º `my-awesome-name/my-awesome-lora`ï¼Œç¾åœ¨æˆ‘å€‘å¯ä»¥ä½¿ç”¨å¾®èª¿æ¨¡å‹é€²è¡Œæ¨ç†ï¼š

```
import torch
from diffusers import CogVideoXPipeline
from diffusers import export_to_video

pipe = CogVideoXPipeline.from_pretrained(
    "THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16
).to("cuda")
+ pipe.load_lora_weights("my-awesome-name/my-awesome-lora", adapter_name=["cogvideox-lora"])
+ pipe.set_adapters(["cogvideox-lora"], [1.0])

video = pipe("<my-awesome-prompt>").frames[0]
export_to_video(video, "output.mp4", fps=8)
```

ä½ ä¹Ÿå¯ä»¥åœ¨[é€™è£¡](tests/test_lora_inference.py)ä¾†æª¢æŸ¥ä½ çš„Loraæ˜¯å¦æ­£å¸¸æ›è¼‰ã€‚

**æ³¨æ„ï¼š** å°æ–¼åœ–åƒåˆ°è¦–é »çš„å¾®èª¿ï¼Œæ‚¨å¿…é ˆå¾ [é€™å€‹åˆ†æ”¯](https://github.com/huggingface/diffusers/pull/9482) å®‰è£
diffusersï¼ˆè©²åˆ†æ”¯ç‚º CogVideoX çš„åœ–åƒåˆ°è¦–é »æ·»åŠ äº† LoRA åŠ è¼‰æ”¯æŒï¼‰ç›´åˆ°å®ƒè¢«åˆä½µã€‚

ä»¥ä¸‹æˆ‘å€‘æä¾›äº†æ›´å¤šæ¢ç´¢æ­¤å„²å­˜åº«é¸é …çš„é¡å¤–éƒ¨åˆ†ã€‚æ‰€æœ‰é€™äº›éƒ½æ—¨åœ¨ç›¡å¯èƒ½é™ä½è¨˜æ†¶é«”éœ€æ±‚ï¼Œä½¿è¦–é »æ¨¡å‹çš„å¾®èª¿è®Šå¾—æ›´æ˜“æ–¼å­˜å–ã€‚

## è¨“ç·´

åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œè«‹ä½ æª¢æŸ¥æ˜¯å¦æŒ‰ç…§[è³‡æ–™é›†è¦ç¯„](assets/dataset_zh.md)æº–å‚™å¥½äº†è³‡æ–™é›†ã€‚ æˆ‘å€‘æä¾›äº†é©ç”¨æ–¼æ–‡æœ¬åˆ°è¦–é » (text-to-video) å’Œåœ–åƒåˆ°è¦–é » (image-to-video) ç”Ÿæˆçš„è¨“ç·´è…³æœ¬ï¼Œå…¼å®¹ [CogVideoX æ¨¡å‹å®¶æ—](https://huggingface.co/collections/THUDM/cogvideo-66c08e62f1685a3ade464cce)ã€‚è¨“ç·´å¯ä»¥é€šé `train*.sh` è…³æœ¬å•Ÿå‹•ï¼Œå…·é«”å–æ±ºæ–¼ä½ æƒ³è¦è¨“ç·´çš„ä»»å‹™ã€‚è®“æˆ‘å€‘ä»¥æ–‡æœ¬åˆ°è¦–é »çš„ LoRA å¾®èª¿ç‚ºä¾‹ã€‚

- æ ¹æ“šä½ çš„éœ€æ±‚é…ç½®ç’°å¢ƒè®Šæ•¸ï¼š

  ```
  export TORCH_LOGS="+dynamo,recompiles,graph_breaks"
  export TORCHDYNAMO_VERBOSE=1
  export WANDB_MODE="offline"
  export NCCL_P2P_DISABLE=1
  export TORCH_NCCL_ENABLE_MONITORING=0
  ```

- é…ç½®ç”¨æ–¼è¨“ç·´çš„ GPUï¼š`GPU_IDS="0,1"`

- é¸æ“‡è¨“ç·´çš„è¶…åƒæ•¸ã€‚è®“æˆ‘å€‘ä»¥å­¸ç¿’ç‡å’Œå„ªåŒ–å™¨é¡å‹çš„è¶…åƒæ•¸éæ­·ç‚ºä¾‹ï¼š

  ```
  LEARNING_RATES=("1e-4" "1e-3")
  LR_SCHEDULES=("cosine_with_restarts")
  OPTIMIZERS=("adamw" "adam")
  MAX_TRAIN_STEPS=("3000")
  ```

- é¸æ“‡ç”¨æ–¼è¨“ç·´çš„ Accelerate é…ç½®æ–‡ä»¶ï¼š`ACCELERATE_CONFIG_FILE="accelerate_configs/uncompiled_1.yaml"`
  ã€‚æˆ‘å€‘åœ¨ `accelerate_configs/` ç›®éŒ„ä¸­æä¾›äº†ä¸€äº›é»˜èªé…ç½® - å–® GPU ç·¨è­¯/æœªç·¨è­¯ã€2x GPU DDPã€DeepSpeed
  ç­‰ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `accelerate config --config_file my_config.yaml` è‡ªå®šç¾©é…ç½®æ–‡ä»¶ã€‚

- æŒ‡å®šå­—å¹•å’Œè¦–é »çš„çµ•å°è·¯å¾‘ä»¥åŠåˆ—/æ–‡ä»¶ã€‚

  ```
  DATA_ROOT="/path/to/my/datasets/video-dataset-disney"
  CAPTION_COLUMN="prompt.txt"
  VIDEO_COLUMN="videos.txt"
  ```

- é‹è¡Œå¯¦é©—ï¼Œéæ­·ä¸åŒçš„è¶…åƒæ•¸ï¼š
    ```
  for learning_rate in "${LEARNING_RATES[@]}"; do
    for lr_schedule in "${LR_SCHEDULES[@]}"; do
      for optimizer in "${OPTIMIZERS[@]}"; do
        for steps in "${MAX_TRAIN_STEPS[@]}"; do
          output_dir="/path/to/my/models/cogvideox-lora__optimizer_${optimizer}__steps_${steps}__lr-schedule_${lr_schedule}__learning-rate_${learning_rate}/"

          cmd="accelerate launch --config_file $ACCELERATE_CONFIG_FILE --gpu_ids $GPU_IDS training/cogvideox_text_to_video_lora.py \
            --pretrained_model_name_or_path THUDM/CogVideoX-5b \
            --data_root $DATA_ROOT \
            --caption_column $CAPTION_COLUMN \
            --video_column $VIDEO_COLUMN \
            --id_token BW_STYLE \
            --height_buckets 480 \
            --width_buckets 720 \
            --frame_buckets 49 \
            --dataloader_num_workers 8 \
            --pin_memory \
            --validation_prompt \"BW_STYLE A black and white animated scene unfolds with an anthropomorphic goat surrounded by musical notes and symbols, suggesting a playful environment. Mickey Mouse appears, leaning forward in curiosity as the goat remains still. The goat then engages with Mickey, who bends down to converse or react. The dynamics shift as Mickey grabs the goat, potentially in surprise or playfulness, amidst a minimalistic background. The scene captures the evolving relationship between the two characters in a whimsical, animated setting, emphasizing their interactions and emotions:::BW_STYLE A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance\" \
            --validation_prompt_separator ::: \
            --num_validation_videos 1 \
            --validation_epochs 10 \
            --seed 42 \
            --rank 128 \
            --lora_alpha 128 \
            --mixed_precision bf16 \
            --output_dir $output_dir \
            --max_num_frames 49 \
            --train_batch_size 1 \
            --max_train_steps $steps \
            --checkpointing_steps 1000 \
            --gradient_accumulation_steps 1 \
            --gradient_checkpointing \
            --learning_rate $learning_rate \
            --lr_scheduler $lr_schedule \
            --lr_warmup_steps 400 \
            --lr_num_cycles 1 \
            --enable_slicing \
            --enable_tiling \
            --optimizer $optimizer \
            --beta1 0.9 \
            --beta2 0.95 \
            --weight_decay 0.001 \
            --max_grad_norm 1.0 \
            --allow_tf32 \
            --report_to wandb \
            --nccl_timeout 1800"
          
          echo "Running command: $cmd"
          eval $cmd
          echo -ne "-------------------- Finished executing script --------------------\n\n"
        done
      done
    done
  done
  ```

è¦äº†è§£ä¸åŒåƒæ•¸çš„å«ç¾©ï¼Œä½ å¯ä»¥æŸ¥çœ‹ [args](./training/args.py) æ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨ `--help` é‹è¡Œè¨“ç·´è…³æœ¬ã€‚

æ³¨æ„ï¼šè¨“ç·´è…³æœ¬å°šæœªåœ¨ MPS ä¸Šæ¸¬è©¦ï¼Œå› æ­¤æ€§èƒ½å’Œè¨˜æ†¶é«”éœ€æ±‚å¯èƒ½èˆ‡ä¸‹é¢çš„ CUDA å ±å‘Šå·®ç•°å¾ˆå¤§ã€‚

## è¨˜æ†¶é«”éœ€æ±‚

[... è¨˜æ†¶é«”éœ€æ±‚è¡¨æ ¼å’Œè©³ç´°å…§å®¹ä¿æŒä¸è®Š ...]

## å¾…è¾¦äº‹é …

- [x] ä½¿è…³æœ¬å…¼å®¹ DDP
- [ ] ä½¿è…³æœ¬å…¼å®¹ FSDP
- [x] ä½¿è…³æœ¬å…¼å®¹ DeepSpeed
- [ ] åŸºæ–¼ vLLM çš„å­—å¹•è…³æœ¬
- [x] åœ¨ `prepare_dataset.py` ä¸­æ”¯æŒå¤šè§£æåº¦/å¹€æ•¸
- [ ] åˆ†ææ€§èƒ½ç“¶é ¸ä¸¦ç›¡å¯èƒ½æ¸›å°‘åŒæ­¥æ“ä½œ
- [ ] æ”¯æŒ QLoRAï¼ˆå„ªå…ˆï¼‰ï¼Œä»¥åŠå…¶ä»–é«˜ä½¿ç”¨ç‡çš„ LoRA æ–¹æ³•
- [x] ä½¿ç”¨ bitsandbytes çš„ç¯€çœè¨˜æ†¶é«”å„ªåŒ–å™¨æ¸¬è©¦è…³æœ¬
- [x] ä½¿ç”¨ CPUOffloadOptimizer ç­‰æ¸¬è©¦è…³æœ¬
- [ ] ä½¿ç”¨ torchao é‡åŒ–å’Œä½ä½è¨˜æ†¶é«”å„ªåŒ–å™¨æ¸¬è©¦è…³æœ¬ï¼ˆç›®å‰åœ¨ AdamWï¼ˆ8/4-bit torchaoï¼‰ä¸Šå ±éŒ¯ï¼‰
- [ ] ä½¿ç”¨ AdamWï¼ˆ8-bit bitsandbytesï¼‰+ CPUOffloadOptimizerï¼ˆå¸¶æœ‰æ¢¯åº¦å¸è¼‰ï¼‰çš„æ¸¬è©¦è…³æœ¬ï¼ˆç›®å‰å ±éŒ¯ï¼‰
- [ ] [Sage Attention](https://github.com/thu-ml/SageAttention)ï¼ˆèˆ‡ä½œè€…åˆä½œæ”¯æŒåå‘å‚³æ’­ï¼Œä¸¦é‡å° A100 é€²è¡Œå„ªåŒ–ï¼‰

> [!é‡è¦]
> ç”±æ–¼æˆ‘å€‘çš„ç›®æ¨™æ˜¯ä½¿è…³æœ¬ç›¡å¯èƒ½ç¯€çœè¨˜æ†¶é«”ï¼Œå› æ­¤æˆ‘å€‘ä¸ä¿è­‰æ”¯æŒå¤š GPU è¨“ç·´ã€‚